# pySQVD

This python module provides a convenience API for SQVD. REST API documentation is included.

> TODO: Remove import url paramter (assets assigned to study should always imported)
> TODO: Add PDF and BW file uploads

## Installation
pySQD requires the `request` module. Compatible with python 2.7 and 3.0+

Install with: `pip install .`

## Features
### DONE
* Authenticate with SQVD (token based)
* GET/POST/DELETE for study,sample,track,panel
* query parameter support for filtering/searching
* upload VCF and BED files (for BAM files use the REST API)
### TODO
* Find tests by state
* Trigger events

## REST endpoints

All endpoints are authenticated and group (user) restricted.
Only users with **_API_** role are being authorised.
Valid methods are GET, POST, DELETE.
GET requests support query paramaters (eg. /api/v1/study?study_name=mystudy)

API root: `/api/v1`

| Resource | Endpoint                   | Methods         | Notes                            |
| -------- | -------------------------- | --------------- | -------------------------------- |
| Study    | /api/v1/study/:id          | GET,POST,DELETE | Studies (tests)                  |
| Sample   | /api/v1/sample/:id         | GET,POST,DELETE | Samples                          |
| Track    | /api/v1/sample/:id         | GET,POST,DELETE | Workflows                        |
| Sample   | /api/v1/sample/:id         | GET,POST,DELETE | Panels and subpanels             |
| VCF      | /api/v1/study/:id/vcf      | POST            | Upload VCF files (VCF v4.2 spec) |
| BED      | /api/v1/study/:id/bed      | POST            | Upload BED files (intervals)     |
| BEDGRAPH | /api/v1/study/:id/bedgraph | POST            | Upload BEDGRAPH (coverage)       |
| BAM      | /api/v1/study/:id/bed      | POST            | Upload BAM files                 |
| JSON     | /api/v1/study/:id/json     | POST            | Upload JSON files                |

## File uploads
### Query parameters

The upload endpoints do not automatically parse and import the files into the database.
As default, all parsers and file processors will run and processed files will be moved into storage (GridFS).

Specific steps can be skipped by supplying keywords
(eg. `http://localhost:3000/api/v1/study/9zu9BHRGZH2DNSLde/vcf?skip=parser`).

keyword    | Step           | File types       | Description
---------- | -------------- | ---------------- | -------
parsing    | parser         | VCF,BAM, BED, BW | Skips any parsing steps
processing | postprocessing | VCF,BAM, BED, BW | Skips all postprocessing
storing    | storage        | VCF,BAM, BED, BW | Skips move to GridFS storage
bw         | processing     | BAM              | prevents generation of BIGWIG file
bai        | processing     | BAM              | prevents indexing of BAM file
chromsizes | processing     | BAM              | prevents generation of chromsizes (bw depends on this!)
tbi        | processing     | VCF              | prevents indexing of VCF file

You can assign a data type to each uploaded file with the `type` query parameter. This is useful if multiple BED files are uploaded.

Uploaded files are renamed to the study's name (`study_name`). To override this, supply a `name` query parameter (without file extension).


All those URL parameters can be generated by passing an options dictorary to the upload method.

### Import parsers

| File     | Import action                                            | Status |
| -------- | -------------------------------------------------------- | ------ |
| VCF      | Insert VCFv4.2 variants into database                    | DONE   |
| BED      | Inserts IntervalSet into database                        | DONE   |
| BEDGRAPH | Calculate coverage over target regions                   | DONE   |
| BAM      | Calculate limit of detection based on MQ,BQ and coverage | DONE   |
| JSON     | Metrics or IGV track definitions                         | DONE   |

#### JSON files
JSON file uploads must contain an array of either:

1. Metric definitions (crimson format) with `source`, `type` and `data` properties.
```json
[
  {
    "source": "/srv/work/analysis/NEURO000/HD798-00/default/metrics/9faca28e.metrics.READS",
    "type": "READS",
    "data": {
      "metrics": {
        "contents": {
          "PASSED_FRAC": "0.976,0.978",
          "LENGTH_STD": "37.7575903936,37.3991878324",
          "PASSED_READS": "2381351,2384599",
          "TILE_MEDIAN_LENGTH_MIN": "143,144",
          "TILE_MEDIAN_LENGTH_MAX": "148,148",
          "LENGTH_MEAN": "120.816121967,121.366211261",
          "TILE_MEDIAN_LENGTH_STD": "1.250,1.104",
          "LENGTH_MEDIAN": "145.0,146.0",
          "TOTAL_READS": "2439144,2439144"
        }
      },
      "header": {
        "flags": "parseTrimlog.py STDIN",
        "time": "Started on: 2021-01-12T22:57:03.671368"
      },
      "histogram": null
    }
  }
]
```

2. IGV track definitons with at least `name`, `url` and `format` properties.
```json
[
  {
    "name": "NA12787",
    "url": "http://localhost:8000/HG001_GRCh37_1_22_v4.2.1_benchmark.vcf.gz",
    "indexURL": "http://localhost:8000/HG001_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi",
    "format": "vcf"
  }
]
```

The latter is ideal to serve large resources that do not need any processing by SQVD. They can for example be served with `python3 -m http.server 8000` or `npx http-server -p 8000`.


### Limitations
All uploads are currently limited to 200Mb. Imports of BED files are size limited as they are stored in a single document in the database (BSON limit 16Mb). Split into multiple files to overcome this.


## Examples

### pySQVD

```
from pysqd import SQVD

# configure the API connection
sqvd = SQVD(username=sys.argv[1], password=sys.argv[2], host='127.0.0.1:3000')

# automatically logs in and out
with sqvd:
    # upload files and assign to study
    vcfFile = '/path/to/vcf/file'
    bedFile = '/path/to/bed/file'
    study_id = "9zu9BHRGZH2DNSLde"
    sqvd.upload([vcfFile,bedFile],study_id)

    # get all studies
    allStudies = sqvd.rest('study')

    # search study by name
    studies = sqvd.rest('study',data={'study_name': study['data']['study_name']})

    # get study by id
    studyById = sqvd.rest('study','GET',"9zu9BHRGZH2DNSLde")

    # create new study and sample
    obj = {
      'study_name': 'swampletest',
      'sample_id': 'swample',
      'panel_id': 'RCGP',
      'panel_version': 4,
      'workflow': 'dna_somatic',
      'subpanels': [ 'SEX', 'SG' ],
      'group': 'haemonc'
    }
    study = sqvd.createStudy(obj)
```

### cURL

Authenticate with the REST API. Returns authtication token and userId:

```
curl localhost:3000/api/v1/login -d "username=yourUsername&password=yourDarkestSecrets"
```

Upload VCF file and assign to study:

```
curl -X POST -H "X-User-Id: JDPqFBtgzWRZvMRiv" -H "X-Auth-Token: XI-KvLyHDLCNx9gpXfdR5eVySht2J2mPXBbf5mj_m05" -H "Content-Type: application/gz" --data-binary "@../../web/imports/api/assets/data/cancer.vcf.gz" http://localhost:3000/api/v1/study/9zu9BHRGZH2DNSLde/vcf
```

### Helper scripts
Upload to SQVD can also be achieved using the helper scripts provided. 

`runLoader.py` allows uploading results from [snappy](https://git.kingspm.uk/kingspm/snappy) secondary analysis workflows. It can easily be run from within the docker container that wraps pySQVD and all dependecies. For example:

```
docker run -it --rm \
-e SQVDHOST=192.168.1.10/sqvd \
-e SQVDUSER=apiuser \
-e SQVDPASS=password \
-e GROUP=molpath \
-e PANEL_ID=RCGP \
-e PANEL_VERSION=5 \
-v /path/to/runfolder:/data \
seglh/pysqvd:molpath /data
```

